# -*- coding: utf-8 -*-
"""HousePriceModelComparison

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mMqFb_C4mUkP4vmfw4OW63PSPOZaZyfz
"""

import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/datasets/melb_data.csv')
df.head(100)

df.columns

df.describe()

Raw_X = df[['Rooms', 'Distance', 'Bathroom', 'Landsize', 'BuildingArea',
       'Lattitude', 'Longtitude', 'Car']]

y = df.Price

Raw_X.describe()

from sklearn.impute import SimpleImputer
import numpy as np
X = Raw_X.copy()


Q1 = df['BuildingArea'].quantile(0.25)
Q3 = df['BuildingArea'].quantile(0.75)
IQR = Q3 - Q1
upper = Q3 + 1.5 * IQR
X['BuildingArea'] = np.where(df['BuildingArea'] > upper, upper, df['BuildingArea'])
X['BuildingArea'] = SimpleImputer(strategy='median').fit_transform(X[['BuildingArea']])

upper = df['Landsize'].quantile(0.99)   # cap at 99th percentile
X['Landsize'] = np.where(X['Landsize'] > upper, upper, X['Landsize'])

X['Car'] = SimpleImputer(strategy='median').fit_transform(X[['Car']])

print(X.isna().sum())

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1)

for nodes in [100, 200, 300, 400, 500, 600, 700, 800, 900]:
  temp_model1 = DecisionTreeRegressor(max_leaf_nodes=nodes, max_depth=None, random_state=1)
  temp_model1.fit(X_train, y_train)
  temp_pred1 = temp_model1.predict(X_val)
  print(f"Nodes: {nodes} -> MAE Val: {mean_absolute_error(y_val, temp_pred1):.2f}, "
          f"R2 Val: {r2_score(y_val, temp_pred1):.3f}")
  print()

for nodes in [540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660]:
  temp_model1 = DecisionTreeRegressor(max_leaf_nodes=nodes, max_depth=None, random_state=1)
  temp_model1.fit(X_train, y_train)
  temp_pred1 = temp_model1.predict(X_val)
  print(f"Nodes: {nodes} -> MAE Val: {mean_absolute_error(y_val, temp_pred1):.2f}, "
          f"R2 Val: {r2_score(y_val, temp_pred1):.3f}")

print("\nStep 2: Optimize max_depth")
for depth in [10, 20, 25, 30, 35, 40, 45, 50]:
    dt = DecisionTreeRegressor(max_leaf_nodes=590, max_depth=depth, random_state=1)
    dt.fit(X_train, y_train)
    val_pred = dt.predict(X_val)
    print(f"Nodes: 590, Depth: {depth} -> MAE Val: {mean_absolute_error(y_val, val_pred):.2f}, "
          f"R2 Val: {r2_score(y_val, val_pred):.3f}")

dt_model = DecisionTreeRegressor(max_leaf_nodes = 590, max_depth=20, random_state=1)
dt_model.fit(X_train, y_train)
dt_pred = dt_model.predict(X_test)
dt_mae = mean_absolute_error(y_test, dt_pred)
dt_r2_score = r2_score(y_test, dt_pred)
print(f"Nodes: 590, Depth: 20 -> MAE Test: {dt_mae:.2f}, R2 Test: {dt_r2_score:.3f}")

print("Random Forest Results:")

for n in [50, 100, 200, 300]:        # number of trees
    for depth in [10, 20, 25, 30, None]:
      temp_model2 = RandomForestRegressor(n_estimators=n, max_depth=depth, random_state=1)
      temp_model2.fit(X_train, y_train)
      temp_pred2 = temp_model2.predict(X_val)
      print(f"n_estimators: {n} with Depth: {depth} -> MAE Val: {mean_absolute_error(y_val, temp_pred2):.2f}, "
          f"R2 Val: {r2_score(y_val, temp_pred2):.3f}")

for feat in [None, 'sqrt', 'log2']:
    temp_model3 = RandomForestRegressor(
        n_estimators=200,
        max_depth=20,
        max_features=feat,
        random_state=1
    )
    temp_model3.fit(X_train, y_train)
    temp_pred3 = temp_model3.predict(X_val)
    print(f"MaxFeat: {feat} -> MAE: {mean_absolute_error(y_val, temp_pred3):.2f}, "
          f"R2: {r2_score(y_val, temp_pred3):.3f}")

rf_model = RandomForestRegressor(n_estimators=200, max_depth=20, max_features='sqrt', random_state=1)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_r2_score = r2_score(y_test, rf_pred)
print(f"n_estimators: 590, Depth: 20 -> MAE Test: {rf_mae:.2f}, R2 Test: {rf_r2_score:.3f}")

from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# Parameter ranges
n_estimators_list = [100, 200, 300]
max_depth_list = [3, 5, 7]

for n in n_estimators_list:
    for depth in max_depth_list:
        temp_model4 = XGBRegressor(
            n_estimators=n,
            max_depth=depth,
            random_state=1,
            verbosity=0  # suppress output
        )
        temp_model4.fit(X_train, y_train)
        pred = temp_model4.predict(X_val)
        mae = mean_absolute_error(y_val, pred)
        r2 = r2_score(y_val, pred)
        print(f"n_estimators: {n}, max_depth: {depth} -> MAE: {mae:.2f}, R2: {r2:.3f}")

xg_model = XGBRegressor(n_estimators=200, max_depth=7, random_state=1)
xg_model.fit(X_train, y_train)
xg_pred = xg_model.predict(X_test)
xg_mae = mean_absolute_error(y_test, xg_pred)
xg_r2_score = r2_score(y_test, xg_pred)
print(f"n_estimators: 200, Depth: 7 -> MAE Test: {xg_mae:.2f}, R2 Test: {xg_r2_score:.3f}")

print(f"DecisionTreeRegressor -> MAE Test: {dt_mae:.2f}, R2 Test: {dt_r2_score:.3f}")
print(f"RandomForestRegressor -> MAE Test: {rf_mae:.2f}, R2 Test: {rf_r2_score:.3f}")
print(f"XGBregressor -> MAE Test: {xg_mae:.2f}, R2 Test: {xg_r2_score:.3f}")

import matplotlib.pyplot as plt
# Model names
models = ['Decision Tree', 'Random Forest', 'XGBoost']

# Test set metrics (replace these with your actual values if different)
mae_values = [dt_mae, rf_mae, xg_mae]       # MAE (lower is better)
r2_values = [dt_r2_score, rf_r2_score, xg_r2_score]  # R² (higher is better)

x = np.arange(len(models))  # positions for the bars
width = 0.35  # width of the bars

fig, ax1 = plt.subplots(figsize=(8,5))

# Plot MAE bars
bars1 = ax1.bar(x - width/2, mae_values, width, label='MAE', color='skyblue')
ax1.set_ylabel('MAE', color='skyblue')
ax1.tick_params(axis='y', labelcolor='skyblue')

# Create a second y-axis for R²
ax2 = ax1.twinx()
bars2 = ax2.bar(x + width/2, r2_values, width, label='R²', color='salmon')
ax2.set_ylabel('R²', color='salmon')
ax2.tick_params(axis='y', labelcolor='salmon')

# X-axis labels
ax1.set_xticks(x)
ax1.set_xticklabels(models)
ax1.set_title('Comparison of ML Models on Test Set')

# Optional: annotate values on bars
def annotate_bars(bars, ax, fmt="{:.2f}"):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(fmt.format(height),
                    xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0,3),  # offset
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)

annotate_bars(bars1, ax1)
annotate_bars(bars2, ax2)

fig.tight_layout()
plt.show()